{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#импорт инструментов\n",
    "\n",
    "import csvtools\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#использование инструмента по навигации в записях дневников\n",
    "\n",
    "dw = csvtools.DumpWrapper('.')\n",
    "notes = dw.notes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#пример выкачивания записей в дни полнолуний и сохранений их в отдельные документы по дням\n",
    "\n",
    "new_moon = ['1936-1-17',   '1936-1-18',   '1936-1-19',\n",
    "             '1936-2-16',   '1936-2-17',   '1936-2-18',\n",
    "             '1936-3-18',   '1936-3-19',   '1936-3-20', \n",
    "             '1936-4-17',   '1936-4-18',   '1936-4-19', \n",
    "             '1936-5-16',   '1936-5-17',   '1936-5-18',\n",
    "             '1936-6-15',   '1936-6-16',   '1936-6-17',\n",
    "             '1936-7-14',  '1936-7-15',   '1936-7-16',\n",
    "             '1936-8-13',  '1936-8-14',  '1936-9-15', \n",
    "             '1936-9-12',  '1936-9-13',  '1936-9-14', \n",
    "             '1936-10-10', '1936-10-11', '1936-10-12', \n",
    "             '1936-11-9', '1936-11-10', '1936-11-11',\n",
    "             '1936-12-8', '1936-12-9', '1936-12-10']\n",
    "\n",
    "for date in new_moon:\n",
    "    d = tuple(map(int, date.split('-')))\n",
    "    print(d)\n",
    "    l = notes.searchDate(d)\n",
    "    with open ('{}.txt'.format(date), 'w', encoding='utf8') as f:\n",
    "        for i in l:\n",
    "            f.write(i.text)\n",
    "            f.flush()\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "topics\n",
      "\n",
      "\n",
      "0\n",
      "0.004*\"вечер\" + 0.003*\"утро\" + 0.003*\"работа\" + 0.003*\"дело\" + 0.002*\"писать\"\n",
      "1\n",
      "0.004*\"вечер\" + 0.003*\"работа\" + 0.003*\"утро\" + 0.003*\"дело\" + 0.003*\"письмо\"\n",
      "2\n",
      "0.004*\"вечер\" + 0.003*\"работа\" + 0.003*\"дело\" + 0.003*\"утро\" + 0.002*\"хороший\"\n",
      "3\n",
      "0.007*\"з\" + 0.005*\"вечер\" + 0.003*\"й\" + 0.003*\"утро\" + 0.003*\"як\"\n",
      "\n",
      "\n",
      "freqs\n",
      "\n",
      "\n",
      "вечер;2782\n",
      "работа;1913\n",
      "утро;1839\n",
      "дело;1709\n",
      "хороший;1581\n",
      "писать;1469\n",
      "новый;1412\n",
      "большой;1385\n",
      "письмо;1362\n",
      "ничто;1334\n",
      "читать;1312\n",
      "получать;1283\n",
      "ночь;1273\n",
      "самый;1267\n",
      "вчера;1229\n",
      "работать;1178\n",
      "жить;1167\n",
      "ходить;1105\n",
      "дом;1084\n",
      "москва;1076\n",
      "слово;1072\n",
      "делать;1056\n",
      "сделать;1053\n",
      "сидеть;1002\n",
      "приезжать;1000\n",
      "ч;964\n",
      "последний;955\n",
      "книга;946\n",
      "написать;938\n",
      "место;936\n",
      "рука;924\n",
      "уходить;914\n",
      "проходить;868\n",
      "понимать;858\n",
      "д;854\n",
      "друг;836\n",
      "должный;832\n",
      "музей;817\n",
      "рассказывать;805\n",
      "русский;802\n",
      "вопрос;802\n",
      "п;801\n",
      "конец;785\n",
      "сторона;773\n",
      "приходиться;773\n",
      "з;768\n",
      "погода;752\n",
      "домой;737\n",
      "любить;716\n",
      "всякий;704\n",
      "\n",
      "\n",
      "bigrams\n",
      "\n",
      "\n",
      "а-ля швейцария\n",
      "а-ля эраст\n",
      "аарон гуревич\n",
      "абажур дом\n",
      "абажур лампа\n",
      "абажур настольный\n",
      "абажур низкий\n",
      "абажур полк\n",
      "абажур распивать\n",
      "абажур серый\n",
      "абажур стул\n",
      "абаз богданов\n",
      "абаз едва\n",
      "абаз пожимать\n",
      "абаз свозить\n",
      "абаза уверять\n",
      "абазята зоологический\n",
      "абашев мать\n",
      "абашев сооружать\n",
      "аббассо иль\n",
      "аббревиатура немой\n",
      "абвиль суасон\n",
      "абгач секретарь\n",
      "абдрахимов открывать\n",
      "абдул гамида\n",
      "абдулов игра\n",
      "абдулов труппа\n",
      "абеба город\n",
      "абелло близкий\n",
      "абелло врасплох\n",
      "абель гидез\n",
      "абель изумлять\n",
      "аберрация блаженство\n",
      "абессалом конец\n",
      "абзац кулуары\n",
      "абзац понравиться\n",
      "абзац посвящать\n",
      "аби барский\n",
      "аби взнать\n",
      "аби вигода\n",
      "аби викидаючиться\n",
      "аби використать\n",
      "аби изредка\n",
      "аби писати\n",
      "аби швидший\n",
      "абиббула заведующий\n",
      "абиббула министр\n",
      "абик трокадеро\n",
      "абиссинец агрессор\n",
      "абиссинец араб\n"
     ]
    }
   ],
   "source": [
    "import codecs,re,os\n",
    "from pymystem3 import Mystem\n",
    "import codecs, os, sys, pickle\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "\n",
    "mystem_object = Mystem()\n",
    "mystem_object.start()\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "ru_stop = get_stop_words('ru')\n",
    "extra_stops = ['свой', 'говорить', 'идти', 'приходить', 'знать', 'пойти', 'становиться', 'давать', 'час', \n",
    "               'какой-то', 'видеть', 'думать', 'р', 'н', 'оставаться', 'начинать', 'взять', 'стоять', 'выходить']\n",
    "\n",
    "def processDir(dir):\n",
    "    doc_set = []\n",
    "    dirlist = os.listdir(dir)\n",
    "    for filenum in range(len(dirlist)):\n",
    "        doc_set.append(processFile(dirlist[filenum]))\n",
    "    return doc_set\n",
    "\n",
    "def frequencies(doc_set):\n",
    "    words = [item for sublist in doc_set for item in sublist]\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    print (\"\\n\\nfreqs\\n\\n\")\n",
    "    for word, frequency in fdist.most_common(50):\n",
    "        print(u'{};{}'.format(word, frequency))\n",
    "    print (\"\\n\\nbigrams\\n\\n\")\n",
    "    finder = BigramCollocationFinder.from_words(words)\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    for word1, word2 in sorted(bigram for bigram, score in scored)[:50]:\n",
    "        print(u'{} {}'.format(word1, word2))\n",
    "        \n",
    "def topic_modeling(doc_set):\n",
    "    dictionary = corpora.Dictionary(doc_set)\n",
    "    corpus = [dictionary.doc2bow(text) for text in doc_set]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, id2word = dictionary, passes=20)\n",
    "    ldamodel[corpus]\n",
    "    print (\"\\n\\ntopics\\n\\n\")\n",
    "    for a in ldamodel.show_topics(num_topics=4, num_words=5):\n",
    "        for i in a:\n",
    "            print (i)\n",
    "            \n",
    "def processFile(name): \n",
    "    doc = []\n",
    "    f = codecs.open('new/'+name, 'r', 'utf-8').read()\n",
    "    clean_doc = []\n",
    "    words = mystem_object.analyze(f)\n",
    "    for word in words:\n",
    "        if word.get('analysis') != None and word.get('analysis') != [] and word['analysis'][0]['lex'] not in ru_stop and word['analysis'][0]['lex'] not in extra_stops:\n",
    "            clean_doc.append(word['analysis'][0]['lex'])\n",
    "    return clean_doc\n",
    "    \n",
    "     \n",
    "doc_set = processDir('new')    \n",
    "topic_modeling(doc_set)\n",
    "frequencies(doc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "topics\n",
      "\n",
      "\n",
      "0\n",
      "0.005*\"вечер\" + 0.004*\"работа\" + 0.003*\"утро\" + 0.003*\"дело\" + 0.003*\"хороший\"\n",
      "1\n",
      "0.003*\"вечер\" + 0.002*\"работа\" + 0.002*\"хороший\" + 0.002*\"утро\" + 0.002*\"самый\"\n",
      "2\n",
      "0.004*\"вечер\" + 0.003*\"работа\" + 0.003*\"дело\" + 0.003*\"утро\" + 0.002*\"читать\"\n",
      "3\n",
      "0.004*\"вечер\" + 0.003*\"утро\" + 0.003*\"дело\" + 0.003*\"работа\" + 0.003*\"большой\"\n",
      "\n",
      "\n",
      "freqs\n",
      "\n",
      "\n",
      "вечер;2647\n",
      "утро;1925\n",
      "работа;1858\n",
      "дело;1758\n",
      "хороший;1493\n",
      "получать;1392\n",
      "большой;1388\n",
      "новый;1362\n",
      "ничто;1356\n",
      "ночь;1340\n",
      "писать;1324\n",
      "письмо;1276\n",
      "самый;1250\n",
      "читать;1218\n",
      "жить;1177\n",
      "вчера;1150\n",
      "работать;1147\n",
      "дом;1112\n",
      "слово;1089\n",
      "ходить;1071\n",
      "делать;1067\n",
      "москва;992\n",
      "сделать;984\n",
      "последний;982\n",
      "ч;971\n",
      "книга;942\n",
      "написать;917\n",
      "место;898\n",
      "приезжать;898\n",
      "д;893\n",
      "сидеть;885\n",
      "русский;879\n",
      "уходить;875\n",
      "проходить;841\n",
      "рука;837\n",
      "вопрос;825\n",
      "понимать;822\n",
      "приходиться;803\n",
      "конец;786\n",
      "музей;770\n",
      "погода;766\n",
      "рассказывать;763\n",
      "должный;758\n",
      "любить;730\n",
      "старый;709\n",
      "сторона;699\n",
      "з;698\n",
      "оказываться;697\n",
      "п;686\n",
      "смотреть;680\n",
      "\n",
      "\n",
      "bigrams\n",
      "\n",
      "\n",
      "а-ля достоевский\n",
      "а-ля франц\n",
      "ааводиться усталость\n",
      "аарон вероятно\n",
      "аахен аи\n",
      "аахен вернуться\n",
      "аахен добираться\n",
      "аахен знаменательный\n",
      "аахен короче\n",
      "аахен налетать\n",
      "аахен попадать\n",
      "аахен проезжать\n",
      "аахен проехать\n",
      "аба продавать\n",
      "аба рисовать\n",
      "аба слушатель\n",
      "абадский базар\n",
      "абадский кантон\n",
      "абажур детский\n",
      "абажур роза\n",
      "абаз белград\n",
      "абаз встречать\n",
      "абаз надувать\n",
      "абаз начальство\n",
      "абаз париж\n",
      "абаза дело\n",
      "абаза лушков\n",
      "абаза начальство\n",
      "абакан км\n",
      "абакумов жаловаться\n",
      "абакумов пообещать\n",
      "абаран аликочак\n",
      "абаран верхний\n",
      "абаран делижанин\n",
      "абаран красивый\n",
      "абаран местечко\n",
      "абаран подниматься\n",
      "абаран ходить\n",
      "абаран церковь\n",
      "абаранка головной\n",
      "абаранский деревня\n",
      "абаранский деревушка\n",
      "абаранский шоссе\n",
      "абассылар ы\n",
      "абашев глаз\n",
      "абашев хороший\n",
      "аббатство монмажур\n",
      "аббеба толпа\n",
      "абгарович орбеть\n",
      "абдрахманов находиться\n"
     ]
    }
   ],
   "source": [
    "import codecs,re,os\n",
    "from pymystem3 import Mystem\n",
    "import codecs, os, sys, pickle\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "\n",
    "mystem_object = Mystem()\n",
    "mystem_object.start()\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "ru_stop = get_stop_words('ru')\n",
    "extra_stops = ['свой', 'говорить', 'идти', 'приходить', 'знать', 'пойти', 'становиться', 'давать', 'час', \n",
    "               'какой-то', 'видеть', 'думать', 'р', 'н', 'оставаться', 'начинать', 'взять', 'стоять', 'выходить']\n",
    "\n",
    "def processDir(dir):\n",
    "    doc_set = []\n",
    "    dirlist = os.listdir(dir)\n",
    "    for filenum in range(len(dirlist)):\n",
    "        if dirlist[filenum] != '.DS_Store':\n",
    "            doc_set.append(processFile(dirlist[filenum]))\n",
    "    return doc_set\n",
    "\n",
    "def frequencies(doc_set):\n",
    "    words = [item for sublist in doc_set for item in sublist]\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    print (\"\\n\\nfreqs\\n\\n\")\n",
    "    for word, frequency in fdist.most_common(50):\n",
    "        print(u'{};{}'.format(word, frequency))\n",
    "    print (\"\\n\\nbigrams\\n\\n\")\n",
    "    finder = BigramCollocationFinder.from_words(words)\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    for word1, word2 in sorted(bigram for bigram, score in scored)[:50]:\n",
    "        print(u'{} {}'.format(word1, word2))\n",
    "        \n",
    "def topic_modeling(doc_set):\n",
    "    dictionary = corpora.Dictionary(doc_set)\n",
    "    corpus = [dictionary.doc2bow(text) for text in doc_set]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, id2word = dictionary, passes=20)\n",
    "    ldamodel[corpus]\n",
    "    print (\"\\n\\ntopics\\n\\n\")\n",
    "    for a in ldamodel.show_topics(num_topics=4, num_words=5):\n",
    "        for i in a:\n",
    "            print (i)\n",
    "            \n",
    "def processFile(name): \n",
    "    doc = []\n",
    "    f = codecs.open('full/'+name, 'r', 'utf-8').read()\n",
    "    clean_doc = []\n",
    "    words = mystem_object.analyze(f)\n",
    "    for word in words:\n",
    "        if word.get('analysis') != None and word.get('analysis') != [] and word['analysis'][0]['lex'] not in ru_stop and word['analysis'][0]['lex'] not in extra_stops:\n",
    "            clean_doc.append(word['analysis'][0]['lex'])\n",
    "    return clean_doc\n",
    "    \n",
    "     \n",
    "doc_set = processDir('full')    \n",
    "topic_modeling(doc_set)\n",
    "frequencies(doc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "topics\n",
      "\n",
      "\n",
      "0\n",
      "0.004*\"вечер\" + 0.003*\"работа\" + 0.003*\"утро\" + 0.003*\"дело\" + 0.003*\"хороший\"\n",
      "1\n",
      "0.003*\"работа\" + 0.003*\"вечер\" + 0.003*\"дело\" + 0.002*\"самый\" + 0.002*\"утро\"\n",
      "2\n",
      "0.004*\"вечер\" + 0.004*\"работа\" + 0.003*\"утро\" + 0.003*\"дело\" + 0.003*\"ночь\"\n",
      "3\n",
      "0.004*\"вечер\" + 0.003*\"утро\" + 0.003*\"з\" + 0.002*\"читать\" + 0.002*\"большой\"\n",
      "\n",
      "\n",
      "freqs\n",
      "\n",
      "\n",
      "вечер;2547\n",
      "утро;1928\n",
      "работа;1922\n",
      "дело;1681\n",
      "хороший;1473\n",
      "большой;1460\n",
      "новый;1378\n",
      "ничто;1350\n",
      "получать;1334\n",
      "ночь;1310\n",
      "читать;1280\n",
      "самый;1267\n",
      "писать;1233\n",
      "письмо;1232\n",
      "вчера;1193\n",
      "жить;1175\n",
      "дом;1140\n",
      "работать;1118\n",
      "ходить;1107\n",
      "слово;1070\n",
      "сделать;1044\n",
      "последний;1023\n",
      "делать;1008\n",
      "москва;986\n",
      "сидеть;979\n",
      "ч;957\n",
      "приезжать;913\n",
      "уходить;907\n",
      "место;897\n",
      "написать;895\n",
      "русский;890\n",
      "рука;881\n",
      "понимать;880\n",
      "проходить;867\n",
      "д;852\n",
      "книга;799\n",
      "друг;799\n",
      "приходиться;791\n",
      "находить;786\n",
      "вопрос;784\n",
      "должный;776\n",
      "сторона;775\n",
      "погода;762\n",
      "з;742\n",
      "конец;736\n",
      "рассказывать;732\n",
      "обед;728\n",
      "п;719\n",
      "старый;715\n",
      "смотреть;705\n",
      "\n",
      "\n",
      "bigrams\n",
      "\n",
      "\n",
      "а-ля пикассо\n",
      "ааа замахать\n",
      "ааа здравствовать\n",
      "ааводиться усталость\n",
      "ааронов жезл\n",
      "аахен брюссель\n",
      "аахен группа\n",
      "аахен дорога\n",
      "аахен никакой\n",
      "аахен отель\n",
      "аахен приезжать\n",
      "аахен рур\n",
      "абажур грасс\n",
      "абажур лампа\n",
      "абажур стол\n",
      "абаз заставлять\n",
      "абай останавливаться\n",
      "абакумов некрасов\n",
      "абамелек лазарев\n",
      "аббревиатура форма\n",
      "абдул гамида\n",
      "абдулов вейланд\n",
      "абеляра сочинять\n",
      "абеляра элоиза\n",
      "абердинский возраст\n",
      "абердинский задремать\n",
      "абердинский находиться\n",
      "абес алый\n",
      "абесалом этери\n",
      "абессалом попадать\n",
      "абзац глава\n",
      "абзац мейерхольд\n",
      "абзац последний\n",
      "аби його\n",
      "аби понедельник\n",
      "аби славолюбство\n",
      "аби цього\n",
      "аби ю\n",
      "абизаться прохожий\n",
      "абик дверь\n",
      "абилова мабут\n",
      "абинский станица\n",
      "абиссинец вообще\n",
      "абиссинец открытие\n",
      "абиссинец сидеть\n",
      "абиссинец факт\n",
      "абиссинец церковь\n",
      "абиссиния август\n",
      "абиссиния война\n",
      "абиссиния геройский\n"
     ]
    }
   ],
   "source": [
    "import codecs,re,os\n",
    "from pymystem3 import Mystem\n",
    "import codecs, os, sys, pickle\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "\n",
    "mystem_object = Mystem()\n",
    "mystem_object.start()\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "ru_stop = get_stop_words('ru')\n",
    "extra_stops = ['свой', 'говорить', 'идти', 'приходить', 'знать', 'пойти', 'становиться', 'давать', 'час', \n",
    "               'какой-то', 'видеть', 'думать', 'р', 'н', 'оставаться', 'начинать', 'взять', 'стоять', 'выходить']\n",
    "\n",
    "def processDir(dir):\n",
    "    doc_set = []\n",
    "    dirlist = os.listdir(dir)\n",
    "    for filenum in range(len(dirlist)):\n",
    "        doc_set.append(processFile(dirlist[filenum]))\n",
    "    return doc_set\n",
    "\n",
    "def frequencies(doc_set):\n",
    "    words = [item for sublist in doc_set for item in sublist]\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    print (\"\\n\\nfreqs\\n\\n\")\n",
    "    for word, frequency in fdist.most_common(50):\n",
    "        print(u'{};{}'.format(word, frequency))\n",
    "    print (\"\\n\\nbigrams\\n\\n\")\n",
    "    finder = BigramCollocationFinder.from_words(words)\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    for word1, word2 in sorted(bigram for bigram, score in scored)[:50]:\n",
    "        print(u'{} {}'.format(word1, word2))\n",
    "        \n",
    "def topic_modeling(doc_set):\n",
    "    dictionary = corpora.Dictionary(doc_set)\n",
    "    corpus = [dictionary.doc2bow(text) for text in doc_set]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, id2word = dictionary, passes=20)\n",
    "    ldamodel[corpus]\n",
    "    print (\"\\n\\ntopics\\n\\n\")\n",
    "    for a in ldamodel.show_topics(num_topics=4, num_words=5):\n",
    "        for i in a:\n",
    "            print (i)\n",
    "            \n",
    "def processFile(name): \n",
    "    doc = []\n",
    "    f = codecs.open('days/'+name, 'r', 'utf-8').read()\n",
    "    clean_doc = []\n",
    "    words = mystem_object.analyze(f)\n",
    "    for word in words:\n",
    "        if word.get('analysis') != None and word.get('analysis') != [] and word['analysis'][0]['lex'] not in ru_stop and word['analysis'][0]['lex'] not in extra_stops:\n",
    "            clean_doc.append(word['analysis'][0]['lex'])\n",
    "    return clean_doc\n",
    "    \n",
    "     \n",
    "doc_set = processDir('days')    \n",
    "topic_modeling(doc_set)\n",
    "frequencies(doc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
